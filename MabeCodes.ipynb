{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dzakypr/MaBe-2025-Challenge/blob/main/MabeCodes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHhMYz2H6Z58",
        "outputId": "2c1398df-72ca-4d6b-fdd8-301964e614e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7UAoBMh52BT",
        "outputId": "16412709-56ef-4c6d-be9c-f9118889252d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filesystem      Size  Used Avail Use% Mounted on\n",
            "overlay         236G   39G  197G  17% /\n",
            "tmpfs            64M     0   64M   0% /dev\n",
            "shm              84G  4.0K   84G   1% /dev/shm\n",
            "/dev/root       2.0G  1.2G  750M  62% /usr/sbin/docker-init\n",
            "tmpfs            84G   13M   84G   1% /var/colab\n",
            "/dev/sda1       242G   41G  201G  17% /kaggle/input\n",
            "tmpfs            84G     0   84G   0% /proc/acpi\n",
            "tmpfs            84G     0   84G   0% /proc/scsi\n",
            "tmpfs            84G     0   84G   0% /sys/firmware\n",
            "drive           236G   49G  188G  21% /content/drive\n"
          ]
        }
      ],
      "source": [
        "!df -h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "of5lEV9Gl1ZA"
      },
      "outputs": [],
      "source": [
        "# %cd \"/content/MaBe-2025-Challenge\"\n",
        "# !git add .\n",
        "# !git commit -m \"Adding stats.pt\"\n",
        "# !git push origin main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2kHKo3Ss03n",
        "outputId": "6bc7c008-a123-4f25-eca3-7b1682b3e95c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGv76YpkRqHD",
        "outputId": "26023111-4666-49f8-91f4-eac73070a902"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'MaBe-2025-Challenge' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git config --global user.email prastiadidzaky@gmail.com\n",
        "!git config --global user.name dzakypr\n",
        "!git clone https://{userdata.get('mabe-toke')}@github.com/dzakypr/MaBe-2025-Challenge.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cudf-cu12 --extra-index-url=https://pypi.nvidia.com\n",
        "!pip install cupy-cuda12x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2GeKDgjqQKQ",
        "outputId": "7033b15a-42db-4810-b654-c608762c53a0"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://pypi.nvidia.com\n",
            "Requirement already satisfied: cudf-cu12 in /usr/local/lib/python3.12/dist-packages (25.10.0)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.12/dist-packages (from cudf-cu12) (6.2.2)\n",
            "Requirement already satisfied: cuda-python<13.0a0,>=12.9.2 in /usr/local/lib/python3.12/dist-packages (from cudf-cu12) (12.9.4)\n",
            "Requirement already satisfied: cuda-toolkit==12.* in /usr/local/lib/python3.12/dist-packages (from cuda-toolkit[nvcc,nvrtc]==12.*->cudf-cu12) (12.9.1)\n",
            "Requirement already satisfied: cupy-cuda12x>=13.6.0 in /usr/local/lib/python3.12/dist-packages (from cudf-cu12) (13.6.0)\n",
            "Requirement already satisfied: fsspec>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from cudf-cu12) (2025.3.0)\n",
            "Requirement already satisfied: libcudf-cu12==25.10.* in /usr/local/lib/python3.12/dist-packages (from cudf-cu12) (25.10.0)\n",
            "Requirement already satisfied: numba-cuda<0.20.0a0,>=0.19.1 in /usr/local/lib/python3.12/dist-packages (from numba-cuda[cu12]<0.20.0a0,>=0.19.1->cudf-cu12) (0.19.1)\n",
            "Requirement already satisfied: numba<0.62.0a0,>=0.60.0 in /usr/local/lib/python3.12/dist-packages (from cudf-cu12) (0.60.0)\n",
            "Requirement already satisfied: numpy<3.0a0,>=1.23 in /usr/local/lib/python3.12/dist-packages (from cudf-cu12) (2.0.2)\n",
            "Requirement already satisfied: nvtx>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from cudf-cu12) (0.2.14)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from cudf-cu12) (25.0)\n",
            "Requirement already satisfied: pandas<2.4.0dev0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from cudf-cu12) (2.2.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from cudf-cu12) (18.1.0)\n",
            "Requirement already satisfied: pylibcudf-cu12==25.10.* in /usr/local/lib/python3.12/dist-packages (from cudf-cu12) (25.10.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from cudf-cu12) (13.9.4)\n",
            "Requirement already satisfied: rmm-cu12==25.10.* in /usr/local/lib/python3.12/dist-packages (from cudf-cu12) (25.10.0)\n",
            "Requirement already satisfied: typing_extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from cudf-cu12) (4.15.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvcc-cu12==12.9.86.* in /usr/local/lib/python3.12/dist-packages (from cuda-toolkit[nvcc,nvrtc]==12.*->cudf-cu12) (12.9.86)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.9.86.* in /usr/local/lib/python3.12/dist-packages (from cuda-toolkit[nvcc,nvrtc]==12.*->cudf-cu12) (12.9.86)\n",
            "Requirement already satisfied: libkvikio-cu12==25.10.* in /usr/local/lib/python3.12/dist-packages (from libcudf-cu12==25.10.*->cudf-cu12) (25.10.0)\n",
            "Requirement already satisfied: librmm-cu12==25.10.* in /usr/local/lib/python3.12/dist-packages (from libcudf-cu12==25.10.*->cudf-cu12) (25.10.0)\n",
            "Requirement already satisfied: rapids-logger==0.1.* in /usr/local/lib/python3.12/dist-packages (from libcudf-cu12==25.10.*->cudf-cu12) (0.1.19)\n",
            "Requirement already satisfied: cuda-bindings~=12.9.4 in /usr/local/lib/python3.12/dist-packages (from cuda-python<13.0a0,>=12.9.2->cudf-cu12) (12.9.4)\n",
            "Requirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.12/dist-packages (from cupy-cuda12x>=13.6.0->cudf-cu12) (0.8.3)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba<0.62.0a0,>=0.60.0->cudf-cu12) (0.43.0)\n",
            "Requirement already satisfied: cuda-core==0.3.* in /usr/local/lib/python3.12/dist-packages (from numba-cuda[cu12]<0.20.0a0,>=0.19.1->cudf-cu12) (0.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12 in /usr/local/lib/python3.12/dist-packages (from numba-cuda[cu12]<0.20.0a0,>=0.19.1->cudf-cu12) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from numba-cuda[cu12]<0.20.0a0,>=0.19.1->cudf-cu12) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cuda-cccl-cu12 in /usr/local/lib/python3.12/dist-packages (from numba-cuda[cu12]<0.20.0a0,>=0.19.1->cudf-cu12) (12.9.27)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<2.4.0dev0,>=2.0->cudf-cu12) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<2.4.0dev0,>=2.0->cudf-cu12) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<2.4.0dev0,>=2.0->cudf-cu12) (2025.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->cudf-cu12) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->cudf-cu12) (2.19.2)\n",
            "Requirement already satisfied: cuda-pathfinder~=1.1 in /usr/local/lib/python3.12/dist-packages (from cuda-bindings~=12.9.4->cuda-python<13.0a0,>=12.9.2->cudf-cu12) (1.3.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->cudf-cu12) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<2.4.0dev0,>=2.0->cudf-cu12) (1.17.0)\n",
            "Requirement already satisfied: cupy-cuda12x in /usr/local/lib/python3.12/dist-packages (13.6.0)\n",
            "Requirement already satisfied: numpy<2.6,>=1.22 in /usr/local/lib/python3.12/dist-packages (from cupy-cuda12x) (2.0.2)\n",
            "Requirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.12/dist-packages (from cupy-cuda12x) (0.8.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGFnFuuuxxo7",
        "outputId": "7ef586f2-c18a-4238-afea-33144e436550"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'feature_eng_gpu' from '/content/MaBe-2025-Challenge/lib/feature_eng_gpu.py'>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "import importlib\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import sys\n",
        "import gc, re, math\n",
        "import itertools\n",
        "from tqdm.auto import tqdm\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from IPython.display import HTML\n",
        "import json\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pickle\n",
        "import glob\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "sys.path.insert(1, \"/content/MaBe-2025-Challenge/lib\")\n",
        "import data_handling\n",
        "import feature_eng\n",
        "import feature_eng_gpu\n",
        "\n",
        "importlib.reload(data_handling)\n",
        "importlib.reload(feature_eng)\n",
        "importlib.reload(feature_eng_gpu)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLOv8T48qjn7",
        "outputId": "dccd11af-324c-43c1-8850-0200dd1bfc2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of data: 8789\n",
            "frequency of the number of mouse valid_ids\n",
            "[1, 2, 3]       7933\n",
            "[1, 2]           846\n",
            "[1, 2, 3, 4]      10\n",
            "Name: count, dtype: int64\n",
            "valid_ids\n",
            "[1, 2, 3]       6368\n",
            "[1, 2]           655\n",
            "[1, 2, 3, 4]       8\n",
            "Name: count, dtype: int64 valid_ids\n",
            "[1, 2, 3]       1565\n",
            "[1, 2]           191\n",
            "[1, 2, 3, 4]       2\n",
            "Name: count, dtype: int64\n",
            "{'no_action': 0, 'allogroom': 1, 'approach': 2, 'attack': 3, 'attemptmount': 4, 'avoid': 5, 'biteobject': 6, 'chase': 7, 'chaseattack': 8, 'climb': 9, 'defend': 10, 'dig': 11, 'disengage': 12, 'dominance': 13, 'dominancegroom': 14, 'dominancemount': 15, 'ejaculate': 16, 'escape': 17, 'exploreobject': 18, 'flinch': 19, 'follow': 20, 'freeze': 21, 'genitalgroom': 22, 'huddle': 23, 'intromit': 24, 'mount': 25, 'rear': 26, 'reciprocalsniff': 27, 'rest': 28, 'run': 29, 'selfgroom': 30, 'shepherd': 31, 'sniff': 32, 'sniffbody': 33, 'sniffface': 34, 'sniffgenital': 35, 'submit': 36, 'tussle': 37}\n"
          ]
        }
      ],
      "source": [
        "DIR = \"/content/drive/MyDrive/data/MABe-mouse-behavior-detection/\"\n",
        "TRAIN_ANNOTATION = DIR + \"train_annotation/\"\n",
        "TRAIN_TRACKING = DIR + \"train_tracking/\"\n",
        "\n",
        "train = pd.read_csv(DIR + \"train.csv\")\n",
        "\n",
        "# drop likely-sleeping MABe22 clips: condition == \"lights on\"\n",
        "train = train.loc[~(train['lab_id'].astype(str).str.contains('MABe22', na=False) &\n",
        "                    train['mouse1_condition'].astype(str).str.lower().eq('lights on'))].copy()\n",
        "\n",
        "train['valid_ids'] = train[['mouse1_strain', 'mouse2_strain', 'mouse3_strain', 'mouse4_strain']].apply(\n",
        "    lambda x: [i + 1 for i, val in enumerate(x) if pd.notna(val)],\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "test = pd.read_csv(DIR + \"test.csv\")\n",
        "test['sleeping'] = (\n",
        "    test['lab_id'].astype(str).str.contains('MABe22', na=False) &\n",
        "    test['mouse1_condition'].astype(str).str.lower().eq('lights on')\n",
        ")\n",
        "test['n_mice'] = 4 - test[['mouse1_strain','mouse2_strain','mouse3_strain','mouse4_strain']].isna().sum(axis=1)\n",
        "\n",
        "body_parts_tracked_list = list(np.unique(train.body_parts_tracked))\n",
        "\n",
        "drop_body_parts = ['headpiece_bottombackleft', 'headpiece_bottombackright', 'headpiece_bottomfrontleft', 'headpiece_bottomfrontright',\n",
        "                   'headpiece_topbackleft', 'headpiece_topbackright', 'headpiece_topfrontleft', 'headpiece_topfrontright',\n",
        "                   'spine_1', 'spine_2', 'tail_middle_1', 'tail_middle_2', 'tail_midpoint']\n",
        "\n",
        "body_parts = ['body_center',\n",
        " 'ear_left',\n",
        " 'ear_right',\n",
        " 'headpiece_bottombackleft',\n",
        " 'headpiece_bottombackright',\n",
        " 'headpiece_bottomfrontleft',\n",
        " 'headpiece_bottomfrontright',\n",
        " 'headpiece_topbackleft',\n",
        " 'headpiece_topbackright',\n",
        " 'headpiece_topfrontleft',\n",
        " 'headpiece_topfrontright',\n",
        " 'lateral_left',\n",
        " 'lateral_right',\n",
        " 'neck',\n",
        " 'nose',\n",
        " 'tail_base',\n",
        " 'tail_midpoint',\n",
        " 'tail_tip',\n",
        " 'hip_left',\n",
        " 'hip_right',\n",
        " 'head',\n",
        " 'forepaw_left',\n",
        " 'forepaw_right',\n",
        " 'hindpaw_left',\n",
        " 'hindpaw_right',\n",
        " 'spine_1',\n",
        " 'spine_2',\n",
        " 'tail_middle_1',\n",
        " 'tail_middle_2']\n",
        "\n",
        "class_list = ['no_action',\n",
        " 'allogroom',\n",
        " 'approach',\n",
        " 'attack',\n",
        " 'attemptmount',\n",
        " 'avoid',\n",
        " 'biteobject',\n",
        " 'chase',\n",
        " 'chaseattack',\n",
        " 'climb',\n",
        " 'defend',\n",
        " 'dig',\n",
        " 'disengage',\n",
        " 'dominance',\n",
        " 'dominancegroom',\n",
        " 'dominancemount',\n",
        " 'ejaculate',\n",
        " 'escape',\n",
        " 'exploreobject',\n",
        " 'flinch',\n",
        " 'follow',\n",
        " 'freeze',\n",
        " 'genitalgroom',\n",
        " 'huddle',\n",
        " 'intromit',\n",
        " 'mount',\n",
        " 'rear',\n",
        " 'reciprocalsniff',\n",
        " 'rest',\n",
        " 'run',\n",
        " 'selfgroom',\n",
        " 'shepherd',\n",
        " 'sniff',\n",
        " 'sniffbody',\n",
        " 'sniffface',\n",
        " 'sniffgenital',\n",
        " 'submit',\n",
        " 'tussle']\n",
        "\n",
        "class_to_id = {label: idx for idx, label in enumerate(class_list)}\n",
        "\n",
        "_sex_cols = [f'mouse{i}_sex' for i in range(1,5)]\n",
        "_train_sex_lut = (train[['video_id'] + _sex_cols].drop_duplicates('video_id')\n",
        "                  .set_index('video_id').to_dict('index'))\n",
        "_test_sex_lut  = (test[['video_id']  + _sex_cols].drop_duplicates('video_id')\n",
        "                  .set_index('video_id').to_dict('index'))\n",
        "_FEATURE_TEMPLATES = {}\n",
        "\n",
        "selected_body_parts = [item for item in body_parts if item not in drop_body_parts]\n",
        "\n",
        "x_train, x_test = train_test_split(train, test_size=0.2, random_state=67)\n",
        "\n",
        "print(f\"Number of data: {len(train)}\")\n",
        "print(f\"frequency of the number of mouse {train['valid_ids'].value_counts()}\")\n",
        "print(x_train['valid_ids'].value_counts(), x_test['valid_ids'].value_counts())\n",
        "print(class_to_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "it7LKa5Pglbu"
      },
      "outputs": [],
      "source": [
        "def load_process_output(data_path, lab_id, video_id, max_frames=None, action_map=None):\n",
        "    tracking_path = os.path.join(data_path, 'train_annotation', lab_id, f'{video_id}.parquet')\n",
        "    if not os.path.exists(tracking_path):\n",
        "        return None\n",
        "    df_events = pd.read_parquet(tracking_path)\n",
        "    if df_events.empty:\n",
        "        return pd.DataFrame(columns=['frame', 'agent_id', 'target_id', 'action'])\n",
        "    if action_map:\n",
        "        df_events = df_events.copy()\n",
        "        df_events['action'] = df_events['action'].map(action_map)\n",
        "        df_events = df_events.dropna(subset=['action'])\n",
        "        df_events['action'] = df_events['action'].astype(int)\n",
        "    starts = df_events['start_frame'].values\n",
        "    stops = df_events['stop_frame'].values\n",
        "    lengths = (stops - starts + 1).astype(int)\n",
        "    all_frames = np.concatenate([\n",
        "        np.arange(s, e + 1) for s, e in zip(starts, stops)\n",
        "    ])\n",
        "    agent_ids = np.repeat(df_events['agent_id'].values, lengths)\n",
        "    target_ids = np.repeat(df_events['target_id'].values, lengths)\n",
        "    actions = np.repeat(df_events['action'].values, lengths)\n",
        "    df_dense = pd.DataFrame({\n",
        "        'frame': all_frames,\n",
        "        'agent_id': agent_ids,\n",
        "        'target_id': target_ids,\n",
        "        'action': actions\n",
        "    })\n",
        "    if max_frames:\n",
        "        df_dense = df_dense[df_dense['frame'] < max_frames]\n",
        "\n",
        "    return df_dense"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "cwsWEQaz7-nF"
      },
      "outputs": [],
      "source": [
        "common_universal_bodyparts_tracked = ['tail_base', 'ear_right', 'ear_left']\n",
        "mov = ['body_center', 'nose', 'tail_base']\n",
        "angle = [['nose', 'tail_base'], ['head', 'spine_1'],['forepaw_left', 'forepaw_right']]\n",
        "dist = ['body_center', 'nose', 'tail_base']\n",
        "dir = ['nose', 'tail_base']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "EjCQLbqnGAvS"
      },
      "outputs": [],
      "source": [
        "def custom_loss(output, target, criterion):\n",
        "    total_loss = 0\n",
        "    for stage_output in output:\n",
        "        total_loss += criterion(stage_output, target)\n",
        "    return total_loss"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XJMr0kAq4iWg"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHM3SmHHt3Wx",
        "outputId": "1e80e281-07b3-44c5-9798-77430e66f658"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3]\n"
          ]
        }
      ],
      "source": [
        "datum = train.iloc[3]\n",
        "print(datum['valid_ids'])\n",
        "datum_labid = datum['lab_id']\n",
        "datum_vidid = datum['video_id']\n",
        "datum_fps = datum['frames_per_second']\n",
        "datum_px_pr_cm = datum['pix_per_cm_approx']\n",
        "datum_n_mice = datum['valid_ids']\n",
        "x = data_handling.load_and_process_video(DIR, datum_labid, datum_vidid, datum_n_mice,datum_px_pr_cm, selected_body_parts, drop_body_parts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "xinZqOU92fi4"
      },
      "outputs": [],
      "source": [
        "def calculate_joint_stats(data, sample_ratio=0.1, save_path=\"stats.pt\"):\n",
        "    \"\"\"\n",
        "    Calculates Global Median and Joint IQR for coordinate data.\n",
        "\n",
        "    Args:\n",
        "        data_dir: Folder containing your .parquet tracking files.\n",
        "        sample_ratio: Fraction of files to use for estimation (to save RAM).\n",
        "        save_path: Where to save the stats dictionary.\n",
        "    \"\"\"\n",
        "\n",
        "    # Randomly sample files if dataset is huge\n",
        "\n",
        "    all_x = []\n",
        "    all_y = []\n",
        "\n",
        "    for i in tqdm(range(len(data))):\n",
        "        try:\n",
        "            datum = data.iloc[i]\n",
        "            datum_labid = datum['lab_id']\n",
        "            datum_vidid = datum['video_id']\n",
        "            datum_fps = datum['frames_per_second']\n",
        "            datum_px_pr_cm = datum['pix_per_cm_approx']\n",
        "            datum_n_mice = datum['valid_ids']\n",
        "            df = data_handling.load_and_process_video(DIR, datum_labid, datum_vidid, datum_n_mice,datum_px_pr_cm, selected_body_parts, drop_body_parts)\n",
        "\n",
        "            if isinstance(df.columns, pd.MultiIndex):\n",
        "                xs = df.xs('x', level='coords', axis=1).values.flatten()\n",
        "                ys = df.xs('y', level='coords', axis=1).values.flatten()\n",
        "            else:\n",
        "                # Flat columns assumption: ends with '_x' or '_y'\n",
        "                x_cols = [c for c in df.columns if c.endswith('_x') or '_x_' in c]\n",
        "                y_cols = [c for c in df.columns if c.endswith('_y') or '_y_' in c]\n",
        "                xs = df[x_cols].values.flatten()\n",
        "                ys = df[y_cols].values.flatten()\n",
        "\n",
        "            # Filter NaNs immediately\n",
        "            all_x.append(xs[~np.isnan(xs)])\n",
        "            all_y.append(ys[~np.isnan(ys)])\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Skipping {f}: {e}\")\n",
        "\n",
        "    # Concatenate everything\n",
        "    cat_x = np.concatenate(all_x)\n",
        "    cat_y = np.concatenate(all_y)\n",
        "\n",
        "    # 1. Calculate Medians (Centering)\n",
        "    # Even if you centered per-video, this fixes any global bias\n",
        "    median_x = np.median(cat_x)\n",
        "    median_y = np.median(cat_y)\n",
        "\n",
        "    # 2. Calculate Joint Scale (IQR)\n",
        "    # We combine X and Y deviations to find a single \"Pixel/CM Scale\" factor\n",
        "    dev_x = np.abs(cat_x - median_x)\n",
        "    dev_y = np.abs(cat_y - median_y)\n",
        "    combined_dev = np.concatenate([dev_x, dev_y])\n",
        "\n",
        "    q25 = np.percentile(combined_dev, 25)\n",
        "    q75 = np.percentile(combined_dev, 75)\n",
        "    iqr = q75 - q25\n",
        "\n",
        "    if iqr == 0: iqr = 1.0 # Safety\n",
        "\n",
        "    stats = {\n",
        "        'median_x': float(median_x),\n",
        "        'median_y': float(median_y),\n",
        "        'scale': float(iqr)\n",
        "    }\n",
        "\n",
        "    print(\"Stats Calculated:\", stats)\n",
        "    torch.save(stats, save_path)\n",
        "    return stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "vW3kKo9HAFW3"
      },
      "outputs": [],
      "source": [
        "def build_agent_centric_training_sample(df_wide, df_dense_labels=None, max_neighbors=3, valid_ids=None):\n",
        "    \"\"\"\n",
        "    Transforms coordinates into Agent-Centric inputs AND aligns labels for training.\n",
        "\n",
        "    CRITICAL CHANGE: This version allows 'Self' to be a neighbor.\n",
        "    This enables training on behaviors like 'Rear' or 'Groom' where target_id == agent_id.\n",
        "    \"\"\"\n",
        "    # 1. Setup IDs\n",
        "    present_ids = df_wide.columns.get_level_values('mouse_id').unique().tolist()\n",
        "    if valid_ids:\n",
        "        valid_ids = [int(v) for v in valid_ids]\n",
        "        present_ids = [int(p) for p in present_ids]\n",
        "        mice_ids = [m for m in present_ids if m in valid_ids]\n",
        "    else:\n",
        "        mice_ids = [int(p) for p in present_ids]\n",
        "    mice_ids.sort()\n",
        "\n",
        "    samples = []\n",
        "    num_frames = len(df_wide)\n",
        "\n",
        "    # Pre-process labels\n",
        "    if df_dense_labels is not None and not df_dense_labels.empty:\n",
        "        df_dense_labels['agent_id'] = df_dense_labels['agent_id'].astype(int)\n",
        "        df_dense_labels['target_id'] = df_dense_labels['target_id'].astype(int)\n",
        "\n",
        "    # 2. Pre-calculate Centroids\n",
        "    mouse_centroids = {}\n",
        "    for m in mice_ids:\n",
        "        xs = df_wide[m].xs('x', level='coords', axis=1)\n",
        "        ys = df_wide[m].xs('y', level='coords', axis=1)\n",
        "        mouse_centroids[m] = (np.nanmedian(xs.values), np.nanmedian(ys.values))\n",
        "\n",
        "    for ego_id in mice_ids:\n",
        "        # --- A. Prepare Ego Features ---\n",
        "        ego_df = df_wide[ego_id].sort_index(axis=1)\n",
        "        ego_df = ego_df.interpolate(method='linear', limit=10, limit_direction='both')\n",
        "        ego_data = ego_df.values\n",
        "\n",
        "        # Mask Generation\n",
        "        valid_counts = np.sum(~np.isnan(ego_data), axis=1)\n",
        "        total_parts = ego_data.shape[1]\n",
        "        valid_frames = valid_counts >= (total_parts * 0.5)\n",
        "        nan_mask = torch.tensor(valid_frames, dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "        # --- B. Sort Neighbors (INCLUDING SELF) ---\n",
        "        distances = []\n",
        "        for n_id in mice_ids:\n",
        "            # REMOVED: if n_id == ego_id: continue  <-- This allows Self-Targeting\n",
        "\n",
        "            e_c, n_c = mouse_centroids[ego_id], mouse_centroids[n_id]\n",
        "\n",
        "            if np.isnan(e_c[0]) or np.isnan(n_c[0]):\n",
        "                dist = 99999.0\n",
        "            else:\n",
        "                dist = np.sqrt((e_c[0]-n_c[0])**2 + (e_c[1]-n_c[1])**2)\n",
        "            distances.append((n_id, dist))\n",
        "\n",
        "        # Sort: Self (Dist 0) will always be first, followed by closest real neighbors\n",
        "        distances.sort(key=lambda x: x[1])\n",
        "        sorted_neighbors = [d[0] for d in distances]\n",
        "\n",
        "        # --- C. Build Feature & Label Blocks ---\n",
        "        neighbor_feature_blocks = []\n",
        "        label_tensor = torch.zeros((max_neighbors, num_frames), dtype=torch.long)\n",
        "\n",
        "        if df_dense_labels is not None and not df_dense_labels.empty:\n",
        "            ego_acts = df_dense_labels[df_dense_labels['agent_id'] == ego_id]\n",
        "        else:\n",
        "            ego_acts = pd.DataFrame()\n",
        "\n",
        "        for i in range(max_neighbors):\n",
        "            if i < len(sorted_neighbors):\n",
        "                n_id = sorted_neighbors[i]\n",
        "\n",
        "                # 1. Neighbor Features\n",
        "                n_df = df_wide[n_id].sort_index(axis=1)\n",
        "                n_df = n_df.interpolate(method='linear', limit=10, limit_direction='both')\n",
        "                neighbor_feature_blocks.append(n_df.values)\n",
        "\n",
        "                # 2. Neighbor Labels\n",
        "                if not ego_acts.empty:\n",
        "                    target_acts = ego_acts[ego_acts['target_id'] == n_id]\n",
        "\n",
        "                    if not target_acts.empty:\n",
        "                        frames = target_acts['frame'].values\n",
        "                        actions = target_acts['action'].values\n",
        "\n",
        "                        valid_idx = frames < num_frames\n",
        "                        frames = frames[valid_idx]\n",
        "                        actions = actions[valid_idx]\n",
        "\n",
        "                        if len(frames) > 0:\n",
        "                            label_tensor[i, frames] = torch.tensor(actions, dtype=torch.long)\n",
        "\n",
        "            else:\n",
        "                # Padding\n",
        "                neighbor_feature_blocks.append(np.zeros_like(ego_data))\n",
        "\n",
        "        # --- D. Final Assembly ---\n",
        "        all_data = np.concatenate([ego_data] + neighbor_feature_blocks, axis=1)\n",
        "        all_data = np.nan_to_num(all_data, nan=0.0)\n",
        "\n",
        "        feature_tensor = torch.tensor(all_data, dtype=torch.float32).T\n",
        "\n",
        "        samples.append({\n",
        "            'agent_id': ego_id,\n",
        "            'features': feature_tensor,\n",
        "            'labels': label_tensor,\n",
        "            'mask': nan_mask,\n",
        "            'neighbors': sorted_neighbors[:max_neighbors]\n",
        "        })\n",
        "\n",
        "    return samples"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_and_save_dataset(\n",
        "    metadata_df,\n",
        "    DIR,\n",
        "    output_dir=\"processed_dataset_chunks\", # Changed: Saves to a directory now\n",
        "    stats_path=\"stats.pt\",\n",
        "    max_neighbors=3,\n",
        "    action_map=None\n",
        "):\n",
        "    \"\"\"\n",
        "    Full pipeline to load, transform, normalize, and save the dataset in CHUNKS.\n",
        "    Saves individual .pkl files per video to prevent RAM crashes (OOM).\n",
        "\n",
        "    Args:\n",
        "        metadata_df: DataFrame containing video metadata (lab_id, video_id, valid_ids, etc.)\n",
        "        data_dir: Root directory of the dataset.\n",
        "        output_dir: Directory where processed chunks will be saved.\n",
        "        stats_path: Path to normalization stats file.\n",
        "        max_neighbors: Number of neighbor slots for the model.\n",
        "        action_map: Dictionary mapping action strings to integers (1-based).\n",
        "    \"\"\"\n",
        "\n",
        "    # Create output directory\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # 1. Load or Calculate Normalization Stats\n",
        "    if os.path.exists(stats_path):\n",
        "        print(f\"Loading stats from {stats_path}...\")\n",
        "        stats = torch.load(stats_path)\n",
        "    else:\n",
        "        print(\"Calculating global stats...\")\n",
        "        # Note: You might want to adjust sample_ratio based on dataset size\n",
        "        stats = calculate_joint_stats(metadata_df, data_dir, sample_ratio=0.1, save_path=stats_path)\n",
        "\n",
        "    print(f\"Normalization Stats: Median X={stats['median_x']:.2f}, Scale={stats['scale']:.2f}\")\n",
        "\n",
        "    # Removed: all_samples = [] (This caused the crash)\n",
        "\n",
        "    print(f\"Starting processing loop. Saving chunks to {output_dir}...\")\n",
        "    count = 0\n",
        "\n",
        "    for i in tqdm(range(len(metadata_df))):\n",
        "        try:\n",
        "            datum = metadata_df.iloc[i]\n",
        "            datum_labid = datum['lab_id']\n",
        "            datum_vidid = datum['video_id']\n",
        "            datum_fps = datum['frames_per_second']\n",
        "            datum_px_pr_cm = datum['pix_per_cm_approx']\n",
        "            datum_n_mice = datum['valid_ids']\n",
        "\n",
        "            # Construct chunk filename\n",
        "            chunk_path = os.path.join(output_dir, f\"{datum_labid}_{datum_vidid}.pkl\")\n",
        "\n",
        "            # Optional: Skip if already exists (resume capability)\n",
        "            # if os.path.exists(chunk_path): continue\n",
        "\n",
        "            # --- A. Load Coordinates ---\n",
        "            # We explicitly ask for 'nose' and 'tail_base' to ensure we have minimal features\n",
        "\n",
        "            df_wide = data_handling.load_and_process_video(DIR,\n",
        "                                                           datum_labid,\n",
        "                                                           datum_vidid,\n",
        "                                                           valid_ids=datum_n_mice,\n",
        "                                                           add_bp=selected_body_parts,\n",
        "                                                           drop_bp=drop_body_parts,\n",
        "                                                           pixel_per_cm=datum_px_pr_cm,)\n",
        "\n",
        "            if df_wide is None or df_wide.empty:\n",
        "                continue\n",
        "\n",
        "            # --- B. Load Annotations ---\n",
        "            # Load sparse events and convert to dense frames\n",
        "            df_sparse_annot = load_process_output(\n",
        "                DIR, # This function likely constructs path internally\n",
        "                datum_labid,\n",
        "                datum_vidid,\n",
        "                # No max_frames limit here, we handle it in building\n",
        "                action_map=action_map\n",
        "            )\n",
        "\n",
        "            # Ensure dense labels (expand_events_to_frames logic needs to be here or inside load_process_output)\n",
        "            # Assuming load_process_output ALREADY returns dense frames based on your previous snippet\n",
        "            df_dense = df_sparse_annot\n",
        "\n",
        "            # --- C. Apply Global Normalization (Manual Step) ---\n",
        "            # We modify the dataframe in-place before transformation\n",
        "\n",
        "            # Apply Scale (Divide)\n",
        "            df_wide = df_wide / stats['scale']\n",
        "\n",
        "            # Apply Centering (Subtract)\n",
        "            df_wide.loc[:, (slice(None), slice(None), 'x')] -= stats['median_x']\n",
        "            df_wide.loc[:, (slice(None), slice(None), 'y')] -= stats['median_y']\n",
        "\n",
        "            # --- D. Build Agent-Centric Samples ---\n",
        "            # This function handles Interpolation, Masking, Neighbor Sorting, and Label Alignment\n",
        "            samples = build_agent_centric_training_sample(\n",
        "                df_wide,\n",
        "                df_dense_labels=df_dense,\n",
        "                max_neighbors=max_neighbors,\n",
        "                # Since we remapped IDs in load_video, valid IDs are now 1..N\n",
        "                valid_ids=list(range(1, len(datum_n_mice) + 1))\n",
        "            )\n",
        "\n",
        "            # --- E. Save Chunk Immediately ---\n",
        "            with open(chunk_path, 'wb') as f:\n",
        "                pickle.dump(samples, f)\n",
        "\n",
        "            count += len(samples)\n",
        "\n",
        "            # --- F. Clear Memory ---\n",
        "            del df_wide\n",
        "            del samples\n",
        "            del df_dense\n",
        "            gc.collect()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {datum_vidid}: {e}\")\n",
        "            continue\n",
        "\n",
        "    print(f\"Processing complete. {count} total samples saved to {output_dir}\")\n",
        "    return output_dir"
      ],
      "metadata": {
        "id": "CSKfzsNk4Me9"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# x = prepare_and_save_dataset(x_test, DIR, output_dir='/content/drive/Shareddrives/Feature/test_data_chunks', stats_path='/content/MaBe-2025-Challenge/stats.pt',action_map=class_to_id)"
      ],
      "metadata": {
        "id": "pC_ZnceS7rSI"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CoordinateDataset(Dataset):\n",
        "    def __init__(self, chunk_dir=None, samples_list=None, drop_empty_ratio=0.0):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            chunk_dir: Path to folder containing .pkl chunks (Strategy 2)\n",
        "            samples_list: Direct list of samples (if passed from memory)\n",
        "        \"\"\"\n",
        "        self.samples = []\n",
        "\n",
        "        if samples_list:\n",
        "            self.samples = samples_list\n",
        "        elif chunk_dir:\n",
        "            print(f\"Loading dataset chunks from {chunk_dir}...\")\n",
        "            files = sorted(glob.glob(os.path.join(chunk_dir, \"*.pkl\")))\n",
        "\n",
        "            for f in tqdm(files):\n",
        "                try:\n",
        "                    with open(f, 'rb') as handle:\n",
        "                        chunk_samples = pickle.load(handle)\n",
        "                        self.samples.extend(chunk_samples)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error loading chunk {f}: {e}\")\n",
        "\n",
        "            if drop_empty_ratio > 0.0:\n",
        "                print(f\"Filtering dataset: Dropping {drop_empty_ratio*100:.1f}% of empty samples...\")\n",
        "            self.samples = self._filter_empty(self.samples, drop_empty_ratio)\n",
        "\n",
        "        print(f\"Final Dataset Size: {len(self.samples)} samples.\")\n",
        "\n",
        "    def _filter_empty(self, samples, ratio):\n",
        "        kept_samples = []\n",
        "        dropped_count = 0\n",
        "\n",
        "        for s in samples:\n",
        "            # Check if sample has ANY active label ( > 0 )\n",
        "            # We check the 'labels' tensor.\n",
        "            # Note: labels are (Neighbors, Time), usually integer types.\n",
        "            if 'labels' in s:\n",
        "                labels = s['labels']\n",
        "                if isinstance(labels, torch.Tensor):\n",
        "                    is_active = (labels > 0).any().item()\n",
        "                else:\n",
        "                    is_active = (np.array(labels) > 0).any()\n",
        "\n",
        "                if is_active:\n",
        "                    # Always keep interesting samples\n",
        "                    kept_samples.append(s)\n",
        "                else:\n",
        "                    # It's an empty sample. Keep it with probability (1 - ratio)\n",
        "                    if random.random() > ratio:\n",
        "                        kept_samples.append(s)\n",
        "                    else:\n",
        "                        dropped_count += 1\n",
        "            else:\n",
        "                # No labels (inference mode), keep everything\n",
        "                kept_samples.append(s)\n",
        "\n",
        "        print(f\"Dropped {dropped_count} empty samples.\")\n",
        "        return kept_samples\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.samples[idx]\n",
        "\n",
        "        # 1. Features\n",
        "        features = sample['features']\n",
        "        if not isinstance(features, torch.Tensor):\n",
        "            features = torch.tensor(features, dtype=torch.float32)\n",
        "\n",
        "        # 2. Labels\n",
        "        if 'labels' in sample and sample['labels'] is not None:\n",
        "            labels = sample['labels']\n",
        "            if not isinstance(labels, torch.Tensor):\n",
        "                labels = torch.tensor(labels, dtype=torch.long)\n",
        "        else:\n",
        "            # Fallback for inference (no labels)\n",
        "            # Assuming standard 3 neighbors\n",
        "            num_neighbors = len(sample.get('neighbors', [0,0,0]))\n",
        "            labels = torch.zeros((num_neighbors, features.shape[1]), dtype=torch.long)\n",
        "\n",
        "        # 3. Mask\n",
        "        if 'mask' in sample and sample['mask'] is not None:\n",
        "            mask = sample['mask']\n",
        "            if not isinstance(mask, torch.Tensor):\n",
        "                mask = torch.tensor(mask, dtype=torch.float32)\n",
        "        else:\n",
        "            mask = torch.ones((1, features.shape[1]), dtype=torch.float32)\n",
        "\n",
        "        return features, labels, mask\n",
        "\n",
        "# ... (Keep the pad_collate and create_dataloader functions from previous response) ...\n",
        "# They work exactly the same way.\n",
        "def pad_collate(batch):\n",
        "    # ... (Same as before) ...\n",
        "    features, labels, masks = zip(*batch)\n",
        "    lengths = [f.shape[1] for f in features]\n",
        "    max_len = max(lengths)\n",
        "    batch_size = len(features)\n",
        "    n_feats = features[0].shape[0]\n",
        "    n_neighbors = labels[0].shape[0]\n",
        "\n",
        "    feat_pad = torch.zeros(batch_size, n_feats, max_len)\n",
        "    label_pad = torch.zeros((batch_size, n_neighbors, max_len), dtype=torch.long)\n",
        "    mask_pad = torch.zeros(batch_size, 1, max_len)\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        l = lengths[i]\n",
        "        feat_pad[i, :, :l] = features[i]\n",
        "        label_pad[i, :, :l] = labels[i]\n",
        "        mask_pad[i, :, :l] = masks[i]\n",
        "\n",
        "    return feat_pad, label_pad, mask_pad\n",
        "\n",
        "def create_dataloader(chunk_dir, batch_size=4, shuffle=True, num_workers=2, drop_empty_ratio=0.0):\n",
        "    # Initialize using the directory\n",
        "    ds = CoordinateDataset(chunk_dir=chunk_dir,drop_empty_ratio=drop_empty_ratio)\n",
        "    loader = DataLoader(\n",
        "        ds,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        num_workers=num_workers,\n",
        "        collate_fn=pad_collate\n",
        "    )\n",
        "    return loader"
      ],
      "metadata": {
        "id": "-X4f2P4b3uq1"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = create_dataloader(\n",
        "    chunk_dir='/content/drive/Shareddrives/Feature/train_data_chunks', # The folder you saved to\n",
        "    batch_size=4,\n",
        "    drop_empty_ratio=0.85\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "839b90f590674d46b934c3a5c7db4acf",
            "0776e42678284a95a8368e4ce9866988",
            "076241fced2440d9aee4cdf702f83294",
            "67bf6d1580b04cd0807a56287bcaadd4",
            "739d3c0d3ba64b9586bf8709dda0ed74",
            "6978f50d99c24b7da23b44c375d01a20",
            "8bdd68fbe34443cfade91bbf05805e4e",
            "800e7ccaa15b48c493c13edad3d2258d",
            "aed6d39d8f884b96b1d1cf07ffa71285",
            "9d1f476f31f4462e9bc867839f699b3d",
            "1554939ec166401a850c33ca985d7a68"
          ]
        },
        "id": "QfYyi1KfLgUm",
        "outputId": "c5241aa3-8461-44de-a5c1-24f0c517ee19"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset chunks from /content/drive/Shareddrives/Feature/train_data_chunks...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/7031 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "839b90f590674d46b934c3a5c7db4acf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtering dataset: Dropping 85.0% of empty samples...\n",
            "Dropped 16753 empty samples.\n",
            "Final Dataset Size: 3693 samples.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/Shareddrives/Feature/train.pkl', 'wb') as handle:\n",
        "  pickle.dump(train_loader, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "metadata": {
        "id": "-EfbtfZRdC9M"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loader = create_dataloader(\n",
        "    chunk_dir='/content/drive/Shareddrives/Feature/test_data_chunks', # The folder you saved to\n",
        "    batch_size=4,\n",
        "    drop_empty_ratio=0.85\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "7c6c2f57bede4cc3bd67f4b202419305",
            "718db1a75ea342a884652b87bb875966",
            "1204aac43f3b42c68155d084eaa3bc9a",
            "7141ad4a87844de7a0e1b2d9aa8c1622",
            "4845d12ce8384a83ad242dc6e5aaa81d",
            "fd88a06fcb2746d9ab40f2239f738b4d",
            "6e8ae2956e7549edac28d99aba440ff6",
            "526814011ab048dbbccfa5fbaa1bf345",
            "cfb25e6b5a544fab81093077c76655cf",
            "23e95249fd544e8d90654e407f151df3",
            "2887a4f39b8a439c8f36a90d47da5611"
          ]
        },
        "id": "Lv_zLY3UHvLz",
        "outputId": "e07befd9-254a-41c2-edcb-9350a7bfdfb5"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset chunks from /content/drive/Shareddrives/Feature/test_data_chunks...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1758 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7c6c2f57bede4cc3bd67f4b202419305"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtering dataset: Dropping 85.0% of empty samples...\n",
            "Dropped 4105 empty samples.\n",
            "Final Dataset Size: 980 samples.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/Shareddrives/Feature/test.pkl', 'wb') as handle:\n",
        "  pickle.dump(test_loader, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "metadata": {
        "id": "fXPPUklZgMdD"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for i in tqdm(range(len(train))):\n",
        "#     datum = train.iloc[i]\n",
        "#     datum_labid = datum['lab_id']\n",
        "#     datum_vidid = datum['video_id']\n",
        "#     datum_fps = datum['frames_per_second']\n",
        "#     datum_px_pr_cm = datum['pix_per_cm_approx']\n",
        "#     datum_n_mice = datum['valid_ids']\n",
        "#     e = load_process_output(DIR, datum_labid, datum_vidid)\n",
        "#     # print(e['action'].unique())\n",
        "#     if e['action'].unique() == None:\n",
        "#         print(\"yes\")"
      ],
      "metadata": {
        "id": "fsQQKb8uuQTj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_loader.__len__()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ocV5l7vM6Xg",
        "outputId": "f0b1066c-e74a-4c4e-d990-136cc83c7100"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "924"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "KJcwE1cZMHT4"
      },
      "outputs": [],
      "source": [
        "def calculate_class_weights(dataloader, num_classes):\n",
        "    \"\"\"\n",
        "    Calculates weights for CrossEntropyLoss based on frame counts per class.\n",
        "    Run this ONCE before training using a DataLoader.\n",
        "\n",
        "    Args:\n",
        "        dataloader: A PyTorch DataLoader providing (features, labels, mask) batches.\n",
        "        num_classes: Total number of classes (including No_Action).\n",
        "    \"\"\"\n",
        "    counts = torch.zeros(num_classes)\n",
        "    print(\"Scanning dataloader for class weights (Frame-based)...\")\n",
        "\n",
        "    # Iterate through batches\n",
        "    for batch in tqdm(dataloader):\n",
        "        _, labels, mask = batch\n",
        "        # labels: (Batch, Neighbors, Time)\n",
        "        # mask: (Batch, 1, Time)\n",
        "\n",
        "        # Expand mask to match labels shape (Batch, Neighbors, Time)\n",
        "        valid_mask = mask.expand_as(labels) > 0.5\n",
        "\n",
        "        # Select only valid frames\n",
        "        valid_labels = labels[valid_mask]\n",
        "\n",
        "        # Accumulate counts\n",
        "        unique, c = torch.unique(valid_labels, return_counts=True)\n",
        "        for u, count in zip(unique, c):\n",
        "            if u < num_classes:\n",
        "                counts[u] += count\n",
        "\n",
        "    print(f\"Frame Counts: {counts.tolist()}\")\n",
        "\n",
        "    # Calculate Inverse Frequency Weights\n",
        "    # weight = Total_Frames / (Num_Classes * Class_Count)\n",
        "    # Add epsilon to counts to avoid div-by-zero if a class is missing\n",
        "    total_frames = counts.sum()\n",
        "    weights = total_frames / (num_classes * (counts + 1e-6))\n",
        "\n",
        "    print(f\"Calculated Weights: {weights.tolist()}\")\n",
        "    return weights"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader.__len__()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wf89IkfwMA1-",
        "outputId": "78b3fbe9-a832-449a-b3df-57a09ece8a1a"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "924"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_weights = calculate_class_weights(train_loader, len(class_to_id))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121,
          "referenced_widgets": [
            "df00af8bac9e4eb49b8da40fdf9f9fb2",
            "4ecb8de39d5c4e2488959ed7d0dfbbbb",
            "62630169bcfe4bc6b0e75d3229d3a4f8",
            "9545cb8d0aaa423b9617db19523adab3",
            "381d1085db744f4d87bbd7e7eb0bdaca",
            "f10f95c64c244959baa706cb0aebe214",
            "56e716fd46f343f7adaf56298d7fee9d",
            "89e5e539aa9e4058a20358fa1e0b0b9d",
            "583b2fd1913c4f7da8a486c822baae7e",
            "0a10eab13f08435aa5df9e1a1d4ef95a",
            "6a8e58d64c3a49d5a79b4d90c10143b3"
          ]
        },
        "id": "DmCelefGL6CR",
        "outputId": "c8e86b53-c155-42b9-d837-f9885bed1684"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scanning dataloader for class weights (Frame-based)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/924 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "df00af8bac9e4eb49b8da40fdf9f9fb2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frame Counts: [24466588.0, 0.0, 3778.0, 20675.0, 0.0, 8826.0, 0.0, 6192.0, 1180.0, 0.0, 0.0, 0.0, 0.0, 25348.0, 0.0, 0.0, 0.0, 1435.0, 0.0, 0.0, 29872.0, 0.0, 0.0, 22420.0, 0.0, 0.0, 46618.0, 12665.0, 0.0, 0.0, 0.0, 0.0, 21243.0, 0.0, 0.0, 33203.0, 3156.0, 0.0]\n",
            "Calculated Weights: [0.026570281013846397, 650084155392.0, 172.07098388671875, 31.44300651550293, 650084155392.0, 73.65557861328125, 650084155392.0, 104.98775482177734, 550.9187622070312, 650084155392.0, 650084155392.0, 650084155392.0, 650084155392.0, 25.6463680267334, 650084155392.0, 650084155392.0, 650084155392.0, 453.02032470703125, 650084155392.0, 650084155392.0, 21.762325286865234, 650084155392.0, 650084155392.0, 28.995725631713867, 650084155392.0, 650084155392.0, 13.944917678833008, 51.329185485839844, 650084155392.0, 650084155392.0, 650084155392.0, 650084155392.0, 30.602275848388672, 650084155392.0, 650084155392.0, 19.579078674316406, 205.9835662841797, 650084155392.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEzL2dWI8Nb1"
      },
      "source": [
        "# Model and its kwargs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "eJ5B3__LWazO"
      },
      "outputs": [],
      "source": [
        "model_args = {\"mov1_channels\": 128, \"mov2_channels\": 256, \"mov1_dropout\": 0.3, \"mov2_dropout\": 0.25,\n",
        "              \"mov1_layers\": 0, \"mov2_layers\": 0,\n",
        "              \"pos1_channels\": 32, \"pos2_channels\": 64, \"pos1_dropout\": 0.25, \"pos2_dropout\": 0.2,\n",
        "              \"pos1_layers\": 0, \"pos2_layers\": 0,\n",
        "              \"tof1_channels\": 256, \"tof2_channels\": 512, \"tof1_dropout\": 0.4, \"tof2_dropout\": 0.3,\n",
        "              \"tof1_layers\": 0, \"tof2_layers\": 0,\n",
        "              \"lstm_hidden_size\": 128, \"gru_hidden_size\": 128, \"gaussian_noise_rate\": 0.1, \"dense_channels\": 32,\n",
        "              \"cls_channels1\": 256, \"cls_dropout1\": 0.2, \"cls_channels2\": 128, \"cls_dropout2\": 0.2,}\n",
        "\n",
        "class SEBlock(nn.Module):\n",
        "    def __init__(self, channels, reduction = 8):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(channels, channels // reduction, bias=True)\n",
        "        self.fc2 = nn.Linear(channels // reduction, channels, bias=True)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, C, L)\n",
        "        se = F.adaptive_avg_pool1d(x, 1).squeeze(-1)      # -> (B, C)\n",
        "        se = F.relu(self.fc1(se), inplace=True)          # -> (B, C//r)\n",
        "        se = self.sigmoid(self.fc2(se)).unsqueeze(-1)    # -> (B, C, 1)\n",
        "        return x * se\n",
        "\n",
        "class ResNetSEBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, wd = 1e-4):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels, out_channels,\n",
        "                               kernel_size=3, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
        "        self.conv2 = nn.Conv1d(out_channels, out_channels,\n",
        "                               kernel_size=3, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
        "        # SE\n",
        "        self.se = SEBlock(out_channels)\n",
        "\n",
        "        if in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv1d(in_channels, out_channels, kernel_size=1,\n",
        "                          padding=0, bias=False),\n",
        "                nn.BatchNorm1d(out_channels)\n",
        "            )\n",
        "        else:\n",
        "            self.shortcut = nn.Identity()\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "class AttentionLayer(nn.Module):\n",
        "    def __init__(self, feature_dim):\n",
        "        super().__init__()\n",
        "        self.score_fn = nn.Linear(feature_dim, 1, bias=True)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, L, F)\n",
        "        score = torch.tanh(self.score_fn(x))     # (B, L, 1)\n",
        "        weights = self.softmax(score.squeeze(-1))# (B, L)\n",
        "        weights = weights.unsqueeze(-1)          # (B, L, 1)\n",
        "        context = x * weights                    # (B, L, F)\n",
        "        return context.sum(dim=1)                # (B, F)\n",
        "\n",
        "class GaussianNoise(nn.Module):\n",
        "    \"\"\"Add Gaussian noise to input tensor\"\"\"\n",
        "    def __init__(self, stddev):\n",
        "        super().__init__()\n",
        "        self.stddev = stddev\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.training:\n",
        "            noise = torch.randn_like(x) * self.stddev\n",
        "            return x + noise\n",
        "        return x\n",
        "\n",
        "class MaBeBackbone(nn.Module):\n",
        "    def __init__(self, position_dim, movement_dim, distance_dim, angle_dim, **kwargs):\n",
        "        super().__init__()\n",
        "        self.movement = nn.Sequential(\n",
        "            self.residual_feature_block(3, kwargs[\"mov1_channels\"], kwargs[\"mov1_layers\"], drop=kwargs[\"mov1_dropout\"]),\n",
        "            self.residual_feature_block(kwargs[\"mov1_channels\"], kwargs[\"mov2_channels\"], kwargs[\"mov2_layers\"], drop=kwargs[\"mov2_dropout\"])\n",
        "        )\n",
        "        self.distance = nn.Sequential(\n",
        "            self.residual_feature_block(4, kwargs[\"mov1_channels\"], kwargs[\"mov1_layers\"], drop=kwargs[\"mov1_dropout\"]),\n",
        "            self.residual_feature_block(kwargs[\"mov1_channels\"], kwargs[\"mov2_channels\"], kwargs[\"mov2_layers\"], drop=kwargs[\"mov2_dropout\"])\n",
        "        )\n",
        "        self.angle = nn.Sequential(\n",
        "            self.residual_feature_block(5, kwargs[\"mov1_channels\"], kwargs[\"mov1_layers\"], drop=kwargs[\"mov1_dropout\"]),\n",
        "            self.residual_feature_block(kwargs[\"mov1_channels\"], kwargs[\"mov2_channels\"], kwargs[\"mov2_layers\"], drop=kwargs[\"mov2_dropout\"])\n",
        "        )\n",
        "\n",
        "        self.pos_m_branch1 = self.init_pos_branch(position_dim//4, **kwargs)\n",
        "        self.pos_m_branch2 = self.init_pos_branch(position_dim//4, **kwargs)\n",
        "        self.pos_m_branch3 = self.init_pos_branch(position_dim//4, **kwargs)\n",
        "        self.pos_m_branch4 = self.init_pos_branch(position_dim//4, **kwargs)\n",
        "\n",
        "        self.some_proj = ResNetSEBlock(in_channels=3*kwargs[\"mov2_channels\"], out_channels=kwargs[\"mov2_channels\"])\n",
        "        self.pos_proj = ResNetSEBlock(in_channels=5*kwargs[\"pos2_channels\"], out_channels=kwargs[\"pos2_channels\"])\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=kwargs['mov2_channels']+kwargs['pos2_channels'],\n",
        "            hidden_size=kwargs['lstm_hidden_size'],\n",
        "            num_layers=1,\n",
        "            batch_first=True,\n",
        "            bidirectional=True\n",
        "        )\n",
        "        self.gru = nn.GRU(\n",
        "            input_size=kwargs['mov2_channels']+kwargs['pos2_channels'],\n",
        "            hidden_size=kwargs['gru_hidden_size'],\n",
        "            num_layers=1,\n",
        "            batch_first=True,\n",
        "            bidirectional=True\n",
        "        )\n",
        "\n",
        "        self.noise = GaussianNoise(kwargs['gaussian_noise_rate'])\n",
        "        self.dense = nn.Sequential(\n",
        "            nn.Linear(kwargs['mov2_channels']+kwargs['pos2_channels'], kwargs['dense_channels']),\n",
        "            nn.ELU()\n",
        "        )\n",
        "\n",
        "        self.attn = AttentionLayer(feature_dim=(kwargs['lstm_hidden_size']+kwargs['gru_hidden_size'])*2+kwargs['dense_channels'])  # lstm + gru + dense\n",
        "\n",
        "    def feature_block(self, in_channels, out_channels, num_layers, pool_size=2, drop=0.3): # unchanged\n",
        "        return nn.Sequential(\n",
        "            *[ResNetSEBlock(in_channels=in_channels, out_channels=in_channels) for i in range(num_layers)],\n",
        "            nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm1d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool1d(pool_size, ceil_mode=True),\n",
        "            nn.Dropout(drop)\n",
        "        )\n",
        "\n",
        "    def residual_feature_block(self, in_channels, out_channels, num_layers, pool_size=2, drop=0.3): # unchanged\n",
        "        return nn.Sequential(\n",
        "            *[ResNetSEBlock(in_channels=in_channels, out_channels=in_channels) for i in range(num_layers)],\n",
        "            ResNetSEBlock(in_channels, out_channels, wd=1e-4),\n",
        "            nn.MaxPool1d(pool_size, ceil_mode=True),\n",
        "            nn.Dropout(drop)\n",
        "        )\n",
        "\n",
        "    def init_pos_branch(self, pos_dim , **kwargs):\n",
        "        pos_branch = nn.Sequential(\n",
        "            self.feature_block(pos_dim, kwargs[\"pos1_channels\"], kwargs[\"pos1_layers\"], drop=kwargs[\"pos1_dropout\"]),\n",
        "            self.feature_block(kwargs[\"pos1_channels\"], kwargs[\"pos2_channels\"], kwargs[\"pos2_layers\"], drop=kwargs[\"pos2_dropout\"]),\n",
        "        )\n",
        "        return pos_branch\n",
        "\n",
        "    def forward(self, position, mov_angle_dist):\n",
        "        mov, rot, dist = mov_angle_dist\n",
        "        mov_feat = self.movement(mov.permute(0, 2, 1))\n",
        "        rot_feat = self.dist(rot.permute(0, 2, 1))\n",
        "        dist_feat = self.angle(dist.permute(0, 2, 1))\n",
        "        main_feat = self.some_proj(torch.cat([mov_feat, rot_feat, dist_feat], dim=1))\n",
        "\n",
        "        pos_m1, pos_m2, pos_m3, pos_m4 = position\n",
        "\n",
        "        pos_m1_feat = self.pos_m_branch1(pos_m1.permute(0, 2, 1))\n",
        "        pos_m2_feat = self.pos_m_branch2(pos_m2.permute(0, 2, 1))\n",
        "        pos_m3_feat = self.pos_m_branch3(pos_m3.permute(0, 2, 1))\n",
        "        pos_m4_feat = self.pos_m_branch4(pos_m4.permute(0, 2, 1))\n",
        "        post_feat = self.pos_proj(torch.cat([pos_m1_feat, pos_m2_feat, pos_m3_feat, pos_m4_feat], dim=1))\n",
        "\n",
        "        feat = torch.cat([main_feat, post_feat], dim=1).permute(0, 2, 1)\n",
        "        lstm_out, _ = self.lstm(feat)\n",
        "        gru_out, _ = self.gru(feat)\n",
        "        dense_out = self.dense(self.noise(feat))\n",
        "\n",
        "        return self.attn(torch.cat([lstm_out, gru_out, dense_out], dim=-1))\n",
        "\n",
        "class MaBeModel(nn.Module):\n",
        "    def __init__(self,position_dim, movement_dim, distance_dim, angle_dim, target_classes_num, **kwargs):\n",
        "        super().__init__()\n",
        "        self.backbone = MaBeBackbone(position_dim, movement_dim, distance_dim, angle_dim, **kwargs)\n",
        "        self.target_classifier = nn.Sequential(\n",
        "            nn.Linear((kwargs['lstm_hidden_size']+kwargs['gru_hidden_size'])*2+kwargs['dense_channels'], kwargs[\"cls_channels1\"]),\n",
        "            nn.BatchNorm1d(kwargs[\"cls_channels1\"]),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(kwargs[\"cls_dropout1\"]),\n",
        "            nn.Linear(kwargs[\"cls_channels1\"], kwargs[\"cls_channels2\"]),\n",
        "            nn.BatchNorm1d(kwargs[\"cls_channels2\"]),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(kwargs[\"cls_dropout2\"]),\n",
        "            nn.Linear(kwargs[\"cls_channels2\"], target_classes_num)\n",
        "        )\n",
        "\n",
        "    def forward(self, position, mov_angle_dist):\n",
        "        feat = self.backbone(position, mov_angle_dist)\n",
        "        targets_y = self.target_classifier(feat)\n",
        "        return targets_y"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SEBlock(nn.Module):\n",
        "    def __init__(self, channels, reduction = 8):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(channels, channels // reduction, bias=True)\n",
        "        self.fc2 = nn.Linear(channels // reduction, channels, bias=True)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, C, L)\n",
        "        se = F.adaptive_avg_pool1d(x, 1).squeeze(-1)      # -> (B, C)\n",
        "        se = F.relu(self.fc1(se), inplace=True)          # -> (B, C//r)\n",
        "        se = self.sigmoid(self.fc2(se)).unsqueeze(-1)    # -> (B, C, 1)\n",
        "        return x * se\n",
        "\n",
        "class ResNetSEBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, wd = 1e-4):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels, out_channels,\n",
        "                               kernel_size=3, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
        "        self.conv2 = nn.Conv1d(out_channels, out_channels,\n",
        "                               kernel_size=3, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
        "        # SE\n",
        "        self.se = SEBlock(out_channels)\n",
        "\n",
        "        if in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv1d(in_channels, out_channels, kernel_size=1,\n",
        "                          padding=0, bias=False),\n",
        "                nn.BatchNorm1d(out_channels)\n",
        "            )\n",
        "        else:\n",
        "            self.shortcut = nn.Identity()\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "class AttentionLayer(nn.Module):\n",
        "    def __init__(self, feature_dim):\n",
        "        super().__init__()\n",
        "        self.score_fn = nn.Linear(feature_dim, 1, bias=True)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, L, F)\n",
        "        score = torch.tanh(self.score_fn(x))     # (B, L, 1)\n",
        "        weights = self.softmax(score.squeeze(-1))# (B, L)\n",
        "        weights = weights.unsqueeze(-1)          # (B, L, 1)\n",
        "        context = x * weights                    # (B, L, F)\n",
        "        return context.sum(dim=1)\n",
        "\n",
        "class DilatedResidualLayer(nn.Module):\n",
        "    def __init__(self, dilation, in_channel, out_channel) -> None:\n",
        "        super(DilatedResidualLayer, self).__init__()\n",
        "        self.conv_dilated = nn.Conv1d(in_channel, out_channel, 3, padding=dilation, dilation=dilation)\n",
        "        self.conv1x1 = nn.Conv1d(in_channel, out_channel, 1)\n",
        "        self.dropout = nn.Dropout()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.conv_dilated(x))\n",
        "        out = self.conv1x1(out)\n",
        "        out = self.dropout(out)\n",
        "        return x + out\n",
        "\n",
        "class SingleStageModel(nn.Module):\n",
        "    def __init__(self, num_layers, num_f_maps, dim, num_classes, num_neighbors=4):\n",
        "        super(SingleStageModel, self).__init__()\n",
        "\n",
        "        self.conv_1x1 = nn.Conv1d(dim, num_f_maps, 1)\n",
        "\n",
        "        self.layers = nn.ModuleList([\n",
        "            DilatedResidualLayer(2 ** i, num_f_maps, num_f_maps)\n",
        "            for i in range(num_layers)\n",
        "        ])\n",
        "        self.conv_out = nn.Conv1d(num_f_maps, num_neighbors * num_classes, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv_1x1(x)\n",
        "        for layer in self.layers:\n",
        "            out = layer(out)\n",
        "        out = self.conv_out(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class MultiStageModel(nn.Module):\n",
        "    def __init__(self, num_stages, num_layers, num_f_maps, dim, num_classes, num_neighbors):\n",
        "        super(MultiStageModel, self).__init__()\n",
        "\n",
        "        self.num_neighbors = num_neighbors\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        self.stage1 = SingleStageModel(num_layers, num_f_maps, dim, num_classes,num_neighbors)\n",
        "\n",
        "        self.stages = nn.ModuleList([\n",
        "            SingleStageModel(num_layers, num_f_maps, num_neighbors * num_classes, num_classes, num_neighbors)\n",
        "            for _ in range(num_stages - 1)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        out = self.stage1(x)\n",
        "        outputs = [self._reshape_out(out)]\n",
        "        for s in self.stages:\n",
        "            B, _, T = out.shape\n",
        "            temp = out.view(B, self.num_neighbors, self.num_classes, T)\n",
        "            in_refinement = F.softmax(temp, dim=2) * mask.unsqueeze(1)\n",
        "            out = s(in_refinement.view(B, -1,T))\n",
        "            outputs.append(self._reshape_out(out))\n",
        "        return outputs\n",
        "\n",
        "    def _reshape_out(self, out):\n",
        "        B,_,T = out.shape\n",
        "        return out.view(B, self.num_neighbors, self.num_classes, T)\n",
        "\n",
        "class MabeModel(nn.Module):\n",
        "    def __init__(self, num_stages, num_layers, num_f_maps, dim, num_classes, num_neighbors):\n",
        "        super(MabeModel, self).__init__()\n",
        "        self.MultiStageModel = MultiStageModel(num_stages, num_layers, num_f_maps, dim, num_classes, num_neighbors)\n",
        "        self.ResnetBlock = nn.Sequential()\n",
        "\n",
        "\n",
        "class TMSELoss(nn.Module):\n",
        "    def __init__(self, threshold=4.0):\n",
        "        super(TMSELoss, self).__init__()\n",
        "        self.threshold = threshold\n",
        "        self.mse = nn.MSELoss(reduction='none')\n",
        "\n",
        "    def forward(self, predictions, batch_mask):\n",
        "        loss = self.mse(torch.log_softmax(predictions[:, :, 1:], dim=1),\n",
        "                        torch.log_softmax(predictions[:, :, :-1], dim=1))\n",
        "\n",
        "        loss = torch.clamp(loss, min=0, max=self.threshold**2)\n",
        "\n",
        "        mask = batch_mask[:, :, 1:]\n",
        "        loss = torch.mean(loss * mask)\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "id": "UicfceOzZEZf"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AS2O3GGcIixk"
      },
      "source": [
        "# e"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TMSELoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Truncated Mean Squared Error Loss.\n",
        "    Penalizes rapid flickering between classes (Smoothness constraint).\n",
        "    \"\"\"\n",
        "    def __init__(self, threshold=4.0):\n",
        "        super(TMSELoss, self).__init__()\n",
        "        self.threshold = threshold\n",
        "        self.mse = nn.MSELoss(reduction='none')\n",
        "\n",
        "    def forward(self, predictions, mask):\n",
        "        # predictions shape: (Batch, Neighbors, Classes, Time)\n",
        "\n",
        "        # We want to smooth along the Time dimension (last dim)\n",
        "        # Compare frame T with frame T-1\n",
        "        # Slice [..., 1:] is frames 1 to End\n",
        "        # Slice [..., :-1] is frames 0 to End-1\n",
        "\n",
        "        # Use Log Softmax for numerical stability\n",
        "        log_probs = torch.log_softmax(predictions, dim=2)\n",
        "\n",
        "        loss = self.mse(log_probs[:, :, :, 1:], log_probs[:, :, :, :-1])\n",
        "\n",
        "        # Clamp the loss so we don't penalize HUGE changes (valid action transitions)\n",
        "        # We only want to penalize \"uncertain jitter\"\n",
        "        loss = torch.clamp(loss, min=0, max=self.threshold**2)\n",
        "\n",
        "        # Apply mask to ignore padding (align mask to shape)\n",
        "        # Mask is (Batch, 1, Time) -> Slice to (Batch, 1, Time-1)\n",
        "        mask_sliced = mask[:, :, 1:]\n",
        "\n",
        "        # Expand mask to match Neighbors/Classes dimensions\n",
        "        # (Batch, 1, 1, Time-1) broadcasting\n",
        "        mask_expanded = mask_sliced.unsqueeze(2)\n",
        "\n",
        "        loss = loss * mask_expanded\n",
        "\n",
        "        return loss.mean()\n",
        "\n",
        "class ActionSegmentationLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Combined Loss for MS-TCN.\n",
        "    Calculates CrossEntropy + TMSE for EVERY stage output.\n",
        "    \"\"\"\n",
        "    def __init__(self, class_weights=None, ignore_index=0, tmse_weight=0.15):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            class_weights: Tensor of weights for imbalanced classes.\n",
        "            ignore_index: The label index to ignore (padding usually 0 or -100).\n",
        "            tmse_weight: How much to prioritize smoothness (0.15 is standard).\n",
        "        \"\"\"\n",
        "        super(ActionSegmentationLoss, self).__init__()\n",
        "        self.ce_loss = nn.CrossEntropyLoss(weight=class_weights, reduction='none', ignore_index=ignore_index)\n",
        "        self.tmse_loss = TMSELoss()\n",
        "        self.tmse_weight = tmse_weight\n",
        "\n",
        "    def forward(self, stage_outputs, targets, mask):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            stage_outputs: List of Tensors [Stage1_Out, Stage2_Out...]\n",
        "                           Each shape: (Batch, Neighbors, Classes, Time)\n",
        "            targets: Tensor (Batch, Neighbors, Time)\n",
        "            mask: Tensor (Batch, 1, Time)\n",
        "        \"\"\"\n",
        "        total_loss = 0\n",
        "\n",
        "        # Flatten Batch and Neighbor dimensions for standard CrossEntropy\n",
        "        # Target shape becomes (Batch * Neighbors, Time)\n",
        "        B, N, T = targets.shape\n",
        "        flat_targets = targets.view(-1, T)\n",
        "\n",
        "        # Prepare Mask for CE (Batch * Neighbors, Time)\n",
        "        flat_mask = mask.expand(B, N, T).reshape(-1, T)\n",
        "\n",
        "        for out in stage_outputs:\n",
        "            # 1. Classification Loss (Cross Entropy)\n",
        "            # Input: (Batch, Neighbors, Classes, Time)\n",
        "            # Reshape to (Batch * Neighbors, Classes, Time)\n",
        "            C = out.shape[2]\n",
        "            flat_out = out.view(B * N, C, T)\n",
        "\n",
        "            # Calculate raw CE loss\n",
        "            ce = self.ce_loss(flat_out, flat_targets)\n",
        "\n",
        "            # Apply Time Mask manually (if ignore_index isn't enough)\n",
        "            # ignore_index handles labels, but mask handles valid video duration\n",
        "            masked_ce = (ce * flat_mask).sum() / (flat_mask.sum() + 1e-6)\n",
        "\n",
        "            # 2. Smoothing Loss (TMSE)\n",
        "            tmse = self.tmse_loss(out, mask)\n",
        "\n",
        "            total_loss += masked_ce + (self.tmse_weight * tmse)\n",
        "\n",
        "        return total_loss"
      ],
      "metadata": {
        "id": "hOejtyXSlj8F"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import datetime\n",
        "import random\n",
        "import copy\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "class DeepLearningEssential(object):\n",
        "    def __init__(self, model, loss_fn, optimizer):\n",
        "        self.model = model\n",
        "        self.loss_fn = loss_fn\n",
        "        self.optimizer = optimizer\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        self.model.to(self.device)\n",
        "\n",
        "        self.train_loader = None\n",
        "        self.val_loader = None\n",
        "        self.writer = None\n",
        "        self.scheduler = None\n",
        "        self.is_batch_lr_scheduler = False\n",
        "        self.clipping = None\n",
        "\n",
        "        self.losses = []\n",
        "        self.val_losses = []\n",
        "        self.learning_rates = []\n",
        "        self.total_epochs = 0\n",
        "\n",
        "        self.visualization = {}\n",
        "        self.handles = {}\n",
        "\n",
        "        self.train_step_fn = self._make_train_step_fn()\n",
        "        self.val_step_fn = self._make_val_step_fn()\n",
        "\n",
        "    def to(self, device):\n",
        "        try:\n",
        "            self.device = device\n",
        "            self.model.to(self.device)\n",
        "        except RuntimeError:\n",
        "            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "            print(f\"Couldn't send it to {device}, sending it to {self.device} instead.\")\n",
        "            self.model.to(self.device)\n",
        "\n",
        "    def set_loaders(self, train_loader, val_loader=None):\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "\n",
        "    def set_tensorboard(self, name, folder=\"runs\"):\n",
        "        suffix = datetime.datetime.now().strftime('%Y%m%d%H%M%S')\n",
        "        self.writer = SummaryWriter(f'{folder}/{name}_{suffix}')\n",
        "\n",
        "    def _make_train_step_fn(self):\n",
        "        def perform_train_step_fn(x, y, mask=None):\n",
        "            self.model.train()\n",
        "\n",
        "            # Handle mask for MS-TCN inputs\n",
        "            if mask is not None:\n",
        "                yhat = self.model(x, mask)\n",
        "                loss = self.loss_fn(yhat, y, mask)\n",
        "            else:\n",
        "                yhat = self.model(x)\n",
        "                loss = self.loss_fn(yhat, y)\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            if self.clipping:\n",
        "                self.clipping()\n",
        "\n",
        "            self.optimizer.step()\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            return loss.item()\n",
        "        return perform_train_step_fn\n",
        "\n",
        "    def _make_val_step_fn(self):\n",
        "        def perform_test_step_fn(x, y, mask=None):\n",
        "            self.model.eval()\n",
        "\n",
        "            if mask is not None:\n",
        "                yhat = self.model(x, mask)\n",
        "                loss = self.loss_fn(yhat, y, mask)\n",
        "            else:\n",
        "                yhat = self.model(x)\n",
        "                loss = self.loss_fn(yhat, y)\n",
        "\n",
        "            return loss.item()\n",
        "        return perform_test_step_fn\n",
        "\n",
        "    def _mini_batch_schedulers(self, frac_epoch):\n",
        "        # Placeholder if not defined elsewhere\n",
        "        if self.scheduler and self.is_batch_lr_scheduler:\n",
        "            self.scheduler.step()\n",
        "\n",
        "    def _epoch_schedulers(self, val_loss):\n",
        "        # Placeholder if not defined elsewhere\n",
        "        if self.scheduler and not self.is_batch_lr_scheduler:\n",
        "            if isinstance(self.scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
        "                self.scheduler.step(val_loss)\n",
        "            else:\n",
        "                self.scheduler.step()\n",
        "\n",
        "    def _mini_batch(self, validation=False):\n",
        "        if validation:\n",
        "            data_loader = self.val_loader\n",
        "            step_fn = self.val_step_fn\n",
        "        else:\n",
        "            data_loader = self.train_loader\n",
        "            step_fn = self.train_step_fn\n",
        "\n",
        "        if data_loader is None:\n",
        "            return None\n",
        "\n",
        "        n_batches = len(data_loader)\n",
        "        mini_batch_losses = []\n",
        "\n",
        "        for i, batch in enumerate(data_loader):\n",
        "            # Handle tuple unpacking for (features, labels, mask) or just (x, y)\n",
        "            if len(batch) == 3:\n",
        "                x_batch, y_batch, mask = batch\n",
        "                x_batch = x_batch.to(self.device)\n",
        "                y_batch = y_batch.to(self.device)\n",
        "                mask = mask.to(self.device)\n",
        "\n",
        "                mini_batch_loss = step_fn(x_batch, y_batch, mask)\n",
        "            else:\n",
        "                x_batch, y_batch = batch\n",
        "                x_batch = x_batch.to(self.device)\n",
        "                y_batch = y_batch.to(self.device)\n",
        "\n",
        "                mini_batch_loss = step_fn(x_batch, y_batch)\n",
        "\n",
        "            mini_batch_losses.append(mini_batch_loss)\n",
        "\n",
        "            if not validation:\n",
        "                self._mini_batch_schedulers(i / n_batches)\n",
        "\n",
        "        loss = np.mean(mini_batch_losses)\n",
        "        return loss\n",
        "\n",
        "    def set_seed(self, seed=42):\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "        torch.manual_seed(seed)\n",
        "        np.random.seed(seed)\n",
        "        random.seed(seed)\n",
        "        try:\n",
        "            self.train_loader.sampler.generator.manual_seed(seed)\n",
        "        except AttributeError:\n",
        "            pass\n",
        "\n",
        "    def train(self, n_epochs, seed=42, patience=None, min_delta=0.0):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            n_epochs: Total epochs to train.\n",
        "            seed: Random seed.\n",
        "            patience: Number of epochs to wait for improvement before stopping.\n",
        "                      If None, early stopping is disabled.\n",
        "            min_delta: Minimum change in validation loss to qualify as an improvement.\n",
        "        \"\"\"\n",
        "        self.set_seed(seed)\n",
        "\n",
        "        # Early Stopping State\n",
        "        best_val_loss = float('inf')\n",
        "        patience_counter = 0\n",
        "        best_model_state = None\n",
        "\n",
        "        for epoch in range(n_epochs):\n",
        "            self.total_epochs += 1\n",
        "\n",
        "            loss = self._mini_batch(validation=False)\n",
        "            self.losses.append(loss)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                val_loss = self._mini_batch(validation=True)\n",
        "                self.val_losses.append(val_loss)\n",
        "\n",
        "            self._epoch_schedulers(val_loss)\n",
        "\n",
        "            if self.writer:\n",
        "                scalars = {'training': loss}\n",
        "                if val_loss is not None:\n",
        "                    scalars.update({'validation': val_loss})\n",
        "\n",
        "                self.writer.add_scalars(main_tag='loss',\n",
        "                                        tag_scalar_dict=scalars,\n",
        "                                        global_step=epoch)\n",
        "\n",
        "            # Print progress\n",
        "            val_str = f\" | Val Loss: {val_loss:.4f}\" if val_loss is not None else \"\"\n",
        "            print(f\"Epoch {epoch+1}/{n_epochs} | Train Loss: {loss:.4f}{val_str}\")\n",
        "\n",
        "            # --- EARLY STOPPING LOGIC ---\n",
        "            if patience is not None and val_loss is not None:\n",
        "                # Check if validation loss improved by at least min_delta\n",
        "                if val_loss < (best_val_loss - min_delta):\n",
        "                    best_val_loss = val_loss\n",
        "                    patience_counter = 0\n",
        "                    # Save best state in memory\n",
        "                    best_model_state = copy.deepcopy(self.model.state_dict())\n",
        "                else:\n",
        "                    patience_counter += 1\n",
        "                    if patience_counter >= patience:\n",
        "                        print(f\"Early stopping triggered at epoch {epoch+1}. Best Val Loss: {best_val_loss:.4f}\")\n",
        "                        # Restore best model\n",
        "                        if best_model_state:\n",
        "                            self.model.load_state_dict(best_model_state)\n",
        "                            print(\"Restored model weights from best epoch.\")\n",
        "                        break\n",
        "\n",
        "        if self.writer:\n",
        "            self.writer.close()"
      ],
      "metadata": {
        "id": "-WaJh5PEk2Vc"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "BATCH_SIZE = 4\n",
        "TIME_STEPS = 1000 # 1000 frames (approx 30 seconds at 30fps)\n",
        "\n",
        "# Using the numbers from your feature engineering\n",
        "NUM_EGO_FEATS = len(selected_body_parts)*2 # for x and y\n",
        "NUM_NEIGHBOR_FEATS = NUM_EGO_FEATS\n",
        "MAX_NEIGHBORS = 3\n",
        "NUM_CLASSES = len(class_list) # [No_Action, Attack, Sniff, Mount]\n",
        "\n",
        "# Calculate total input channels (C)\n",
        "# 26 + (24 * 3) = 98 channels\n",
        "INPUT_CHANNELS = NUM_EGO_FEATS + (NUM_NEIGHBOR_FEATS * MAX_NEIGHBORS)\n",
        "\n",
        "# --- 2. Instantiate the Model ---\n",
        "modelTCN = MultiStageModel(\n",
        "    num_stages=4,           # 4 stages (1 prediction + 3 refinement)\n",
        "    num_layers=10,          # 10 layers per stage\n",
        "    num_f_maps=64,          # Hidden dimension size\n",
        "    dim=INPUT_CHANNELS,     # 98\n",
        "    num_classes=NUM_CLASSES,# 4\n",
        "    num_neighbors=MAX_NEIGHBORS # 3 output heads\n",
        ")\n",
        "\n",
        "loss_func = ActionSegmentationLoss(class_weights).to('cuda')\n",
        "optimizer = optim.Adam(modelTCN.parameters(),lr=5e-4, weight_decay=1e-4)\n",
        "\n",
        "# --- 3. Create Dummy Data (The Input) ---\n",
        "# Input: (Batch, Channels, Time)\n",
        "feat,label,mask = next(iter(train_loader))\n",
        "\n",
        "print(f\"Input Shape: {feat.shape}\")\n",
        "print(f\"Mask Shape:  {mask.shape}\")\n",
        "\n",
        "# --- 4. Forward Pass ---\n",
        "# The model returns a list of outputs (one for each stage)\n",
        "# stage_outputs = model(feat, mask)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kdQ_obhiLGJ",
        "outputId": "322de13d-3e78-41a0-881e-d7fd8e0df1f0"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Shape: torch.Size([4, 128, 17119])\n",
            "Mask Shape:  torch.Size([4, 1, 17119])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sigma = DeepLearningEssential(modelTCN, loss_func, optimizer=optimizer)"
      ],
      "metadata": {
        "id": "KbDcVCBwlqhJ"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "schedulere = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer,\n",
        "    mode='min',\n",
        "    factor=0.5,\n",
        "    patience=3,\n",
        ")\n",
        "\n",
        "sigma.set_loaders(train_loader,test_loader)\n",
        "sigma.scheduler = schedulere"
      ],
      "metadata": {
        "id": "0yYmv6BCnH6f"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clear_gpu_memory():\n",
        "    \"\"\"Safely clear PyTorch GPU memory.\"\"\"\n",
        "    try:\n",
        "        gc.collect()  # Run Python garbage collector\n",
        "        torch.cuda.empty_cache()  # Release unreferenced memory\n",
        "        torch.cuda.ipc_collect()  # Collect inter-process memory\n",
        "        print(\" GPU memory cleared.\")\n",
        "    except Exception as e:\n",
        "        print(f\" Error clearing GPU memory: {e}\")\n",
        "\n",
        "# Example usage after training\n",
        "clear_gpu_memory()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68ZZiqT3p7Lh",
        "outputId": "6875a400-f226-4d3c-9f92-ead838c16fce"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " GPU memory cleared.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9KgLugoWpVu8",
        "outputId": "2fc8c0ec-b26f-411b-e56f-22eef68639ab"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Dec 15 20:15:55 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-80GB          Off |   00000000:00:05.0 Off |                    0 |\n",
            "| N/A   33C    P0             77W /  400W |    9031MiB /  81920MiB |    100%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sigma.train(n_epochs=200, patience=7, min_delta=0.01)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "8P7YPmAsn-fC",
        "outputId": "ca3b7626-8d1a-40e2-e873-0d3dada85510"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 1.01 GiB. GPU 0 has a total capacity of 79.32 GiB of which 635.88 MiB is free. Process 18193 has 78.69 GiB memory in use. Of the allocated memory 76.65 GiB is allocated by PyTorch, and 1.55 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1744272967.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msigma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_delta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-3043015248.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, n_epochs, seed, patience, min_delta)\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_epochs\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mini_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3043015248.py\u001b[0m in \u001b[0;36m_mini_batch\u001b[0;34m(self, validation)\u001b[0m\n\u001b[1;32m    121\u001b[0m                 \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m                 \u001b[0mmini_batch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3043015248.py\u001b[0m in \u001b[0;36mperform_train_step_fn\u001b[0;34m(x, y, mask)\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m                 \u001b[0myhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myhat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                 \u001b[0myhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4225836003.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, stage_outputs, targets, mask)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0;31m# 2. Smoothing Loss (TMSE)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m             \u001b[0mtmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mmasked_ce\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtmse_weight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtmse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4225836003.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, predictions, mask)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# Clamp the loss so we don't penalize HUGE changes (valid action transitions)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# We only want to penalize \"uncertain jitter\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# Apply mask to ignore padding (align mask to shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.01 GiB. GPU 0 has a total capacity of 79.32 GiB of which 635.88 MiB is free. Process 18193 has 78.69 GiB memory in use. Of the allocated memory 76.65 GiB is allocated by PyTorch, and 1.55 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "SEzL2dWI8Nb1",
        "AS2O3GGcIixk"
      ],
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "839b90f590674d46b934c3a5c7db4acf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0776e42678284a95a8368e4ce9866988",
              "IPY_MODEL_076241fced2440d9aee4cdf702f83294",
              "IPY_MODEL_67bf6d1580b04cd0807a56287bcaadd4"
            ],
            "layout": "IPY_MODEL_739d3c0d3ba64b9586bf8709dda0ed74"
          }
        },
        "0776e42678284a95a8368e4ce9866988": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6978f50d99c24b7da23b44c375d01a20",
            "placeholder": "",
            "style": "IPY_MODEL_8bdd68fbe34443cfade91bbf05805e4e",
            "value": "100%"
          }
        },
        "076241fced2440d9aee4cdf702f83294": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_800e7ccaa15b48c493c13edad3d2258d",
            "max": 7031,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_aed6d39d8f884b96b1d1cf07ffa71285",
            "value": 7031
          }
        },
        "67bf6d1580b04cd0807a56287bcaadd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9d1f476f31f4462e9bc867839f699b3d",
            "placeholder": "",
            "style": "IPY_MODEL_1554939ec166401a850c33ca985d7a68",
            "value": "7031/7031[1:34:26&lt;00:00,2.51s/it]"
          }
        },
        "739d3c0d3ba64b9586bf8709dda0ed74": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6978f50d99c24b7da23b44c375d01a20": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8bdd68fbe34443cfade91bbf05805e4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "800e7ccaa15b48c493c13edad3d2258d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aed6d39d8f884b96b1d1cf07ffa71285": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9d1f476f31f4462e9bc867839f699b3d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1554939ec166401a850c33ca985d7a68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7c6c2f57bede4cc3bd67f4b202419305": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_718db1a75ea342a884652b87bb875966",
              "IPY_MODEL_1204aac43f3b42c68155d084eaa3bc9a",
              "IPY_MODEL_7141ad4a87844de7a0e1b2d9aa8c1622"
            ],
            "layout": "IPY_MODEL_4845d12ce8384a83ad242dc6e5aaa81d"
          }
        },
        "718db1a75ea342a884652b87bb875966": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fd88a06fcb2746d9ab40f2239f738b4d",
            "placeholder": "",
            "style": "IPY_MODEL_6e8ae2956e7549edac28d99aba440ff6",
            "value": "100%"
          }
        },
        "1204aac43f3b42c68155d084eaa3bc9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_526814011ab048dbbccfa5fbaa1bf345",
            "max": 1758,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cfb25e6b5a544fab81093077c76655cf",
            "value": 1758
          }
        },
        "7141ad4a87844de7a0e1b2d9aa8c1622": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_23e95249fd544e8d90654e407f151df3",
            "placeholder": "",
            "style": "IPY_MODEL_2887a4f39b8a439c8f36a90d47da5611",
            "value": "1758/1758[06:13&lt;00:00,2.73it/s]"
          }
        },
        "4845d12ce8384a83ad242dc6e5aaa81d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd88a06fcb2746d9ab40f2239f738b4d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e8ae2956e7549edac28d99aba440ff6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "526814011ab048dbbccfa5fbaa1bf345": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cfb25e6b5a544fab81093077c76655cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "23e95249fd544e8d90654e407f151df3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2887a4f39b8a439c8f36a90d47da5611": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "df00af8bac9e4eb49b8da40fdf9f9fb2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4ecb8de39d5c4e2488959ed7d0dfbbbb",
              "IPY_MODEL_62630169bcfe4bc6b0e75d3229d3a4f8",
              "IPY_MODEL_9545cb8d0aaa423b9617db19523adab3"
            ],
            "layout": "IPY_MODEL_381d1085db744f4d87bbd7e7eb0bdaca"
          }
        },
        "4ecb8de39d5c4e2488959ed7d0dfbbbb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f10f95c64c244959baa706cb0aebe214",
            "placeholder": "",
            "style": "IPY_MODEL_56e716fd46f343f7adaf56298d7fee9d",
            "value": "100%"
          }
        },
        "62630169bcfe4bc6b0e75d3229d3a4f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_89e5e539aa9e4058a20358fa1e0b0b9d",
            "max": 924,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_583b2fd1913c4f7da8a486c822baae7e",
            "value": 924
          }
        },
        "9545cb8d0aaa423b9617db19523adab3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0a10eab13f08435aa5df9e1a1d4ef95a",
            "placeholder": "",
            "style": "IPY_MODEL_6a8e58d64c3a49d5a79b4d90c10143b3",
            "value": "924/924[00:47&lt;00:00,46.90it/s]"
          }
        },
        "381d1085db744f4d87bbd7e7eb0bdaca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f10f95c64c244959baa706cb0aebe214": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56e716fd46f343f7adaf56298d7fee9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "89e5e539aa9e4058a20358fa1e0b0b9d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "583b2fd1913c4f7da8a486c822baae7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0a10eab13f08435aa5df9e1a1d4ef95a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a8e58d64c3a49d5a79b4d90c10143b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}